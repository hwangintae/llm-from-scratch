# 대규모 언어 모델 이해하기
- LLM은 사람의 언어를 이해하고, 생성하고, 해석하는 능력을 가지고 있다.
- 여기서 '이해'는 일관성 있고 맥락에 맞도록 텍스트를 처리하고 생성한다는 의미이지, 사람과 같은 의식이나 이해력은 아니다.
- LLM의 성공은 Transformer 구조와 방대한 훈련 데이터 덕분이다.


## 1.1 LLM이란 무엇인가요?
- '대규모 언어 모델'에서 '대규모'는 모델의 파라미터 크기와 대량의 훈련 데이터셋을 의미한다.
- 모델 파라미터는 시퀀스의 다음 단어를 예측하도록 훈련하는 신경망의 가중치다.
- 다음 언어 예측은 언어의 고유한 순차 특징을 사용하여 텍스트 안의 맥락, 구조, 관게를 이해하는 모델을 훈련시키는 합리적인 방법이다.
- LLM은 Transformer 구조를 활용하여 사람의 언어에 뉘앙스와 복잡성을 다루는 데 능숙하다.

## 1.2 LLM application
- LLM은 텍스트 분석과 생성에 관련된 작업을 자동하는 것에 유용하다.
- 모델을 혁신하고 새로운 사용 방법에 따라 LLM은 기술과 사람의 관계를 재정의할 수 있다.

## 1.3 LLM의 구축 단계
- LLM은 사전 훈련과 미세 튜닝으로 구성된다.
- 사전 훈련 : 언어에 대한 폭넓은 이해를 쌓는 단계
- 미세 튜닝 : 구체적인 작업이나 특정 데이터셋에서 훈련
  - 지시 미세 튜닝(instruction fine-tuning)
    - 텍스트를 번역하기 위한 쿼리와 올바르게 번역된 텍스트
  - 분류 미세 튜닝(classification fine-tuning)
    - 이메일과 '스팸'/'스팸 아님' 레이블
> 사전 훈련된 모델을 파운데이션 모델, 베이스 모델이라고 부른다.

## 1.4 Transformer 구조 소개
- Transformer는 encoder와 decoder로 구성된다.
- encoder : 입력의 문맥 정보를 포착하는 일련의 수치 표현 또는 vector로 encoding한다.
- decoder : encoding된 vector를 받아 출력 텍스트를 생성한다.
- 셀프 어텐션을 통해 모델은 시퀀스에 있는 서로 다른 단어 또는 토큰에 상대적인 가중치를 부여할 수 있다.
- 셀프 어텐션 덕분에 일관성 있고 맥락에 맞는 출력을 생성할 수 있다.

|           | 입력                                           | 출력                              |
|-----------|----------------------------------------------|---------------------------------|
| 텍스트 완성    | Breakfast is the                             | most important meal of the day. |
| zero-shot | Translate English to German: breakfast =>    | Frühstück                       |
| few-shot  | gaot => goat <br> sheo => shoe <br> pohne => | phone                           |

## 1.5 대규모 데이터셋 활용하기
- GPT 및 BERT의 데이터셋은 수십억 개의 단어가 들어있는 다양하고 광범위한 텍스트 말뭉치로 구성된다.
- 데이터셋의 크기와 다양성으로 모델이 언어 구문, 문법, 맥락에 관련된 다양한 작업과 일반 지식이 필요한 작업도 수행 가능
- LLM을 사전 훈련하려면 상당한 비용이 필요하다. (GPT-3, 460만 달러 추정)

## 1.6 GPT 구조 자세히 살펴보기
- GPT 구조는 비교적 간단하게 decoder 모듈만 사용한다.
- decoder 기반의 모델은 한 번에 한 단어씩 예측하여 텍스트를 생성하기 때문에 자기회귀 모델의 유형이다.
- GPT에서는 이전 시퀀스를 기바능로 다음 단어를 선택하여 일관성을 향상시킨다.
- GPT-3는 96개의 Transformer 층이 있으며 총 1,750억 개의 param을 가진다.
- 구체적으로 번역을 위해 훈련되지 않았지만 언어 사이의 번역 패턴을 학습하고 번역 작업을 수행할 수 있다는 사실은 LLM의 장점과 능력을 잘 보여 준다.

## 1.7 대규모 언어 모델 만들기
- 1단계
  - 데이터 전처리
  - Attention 메커니즘 구현
- 2단계
  - 새로운 텍스트를 생성할 수 있는 LLM을 구현
  - 사전 훈련
  - LLM 평가
- 3단계
  - LLM을 미세 튜닝